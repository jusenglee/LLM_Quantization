<h2>LLM 테스트 서버 제작기</h2>
최근 LLM을 통한 서비스가 많이 나옴에 따라, 현재 내가 다니고 있는 사내에서도 LLM을 이용하여 직접 서비스를 만들기로 결정했다.
현재 해당 프로젝트의 목적은 논문과 관련된 서비스를 운영중인데, 각 기능의 요구사항에 맞는 모델을 결정하기 위해 고객사에서 LLM을 테스트 해볼 수 있는 간단한 기능 페이지를 관리자페이지에 제공해줄것을 요구하여
해당 기능을 간단한 Fast Api를 통한 서버를 만들어 제공하기로 했다.
기존 프로젝트의 경우 ubunt 기반의 서버에서 java로 이루어진 프로젝트를 war 형태로 tomcat에 올려서 제공하는데, gpu 서버와 연결하기위해 gpu서버에 py형태로 python 서버를 올리고, 스프링에서 중간 컨트롤러 파이썬 서버에서 요청과 응답을 받아오는 형태로
만들어서 해당 기능을 구현하였다.

<h3>속도 최적화</h3>
현 gpu 서버의 경우 그래픽카드를 Nvidia의 A40을 장착하였으며, 메모리는 48GB 정도 된다.
일반 가정용 그래픽카드보다는 용량이 널널하지만, 대형 언어모델을 가동시킬 경우 생각보다 용량이 넉넉하진 않았다.
속도 테스트를 직접 해볼경우, 해당 컨트롤러가 요청을 리턴하기 위해서 모델을 로드하고 응답을 생성하는 시간까지 고려 할 경우 평균적으로 약 5분정도 걸렸는데, 아무래도 모델을 미리 로드하지 않고, 요청시마다 직접 로드하는 형태기에 오래걸리는게 큰 것 같다.
그렇다면 모델을 미리 로드한 후 연산을 시키면 그만이지만, 현재 모델 하나를 로드할때 사용되는 메모리 사용량은 이미 40GB를 차지하게 된다.

<h4>양자화</h4>
해당 방안에는 양자화가 가장 적절하다 생각되었다. 모델의 용량을 줄일 수 있으며, LLM 특성상 저용량 모델의 경우 연산속도 또한 빠르다.
해당 방안에는 여러가지 양자화 방법이 있지만 개발 편의성을 위하여 llama.cpp를 통하여 8bit 양자화를 진행하였다.

> <h5>사용된 gguf 변환 명령어</h5>

<pre>
<code>
  python ./convert_hf_to_gguf.py  ../모델이 존재하는 경로/ --outfile 작명할 모델명.gguf --outtype q8_0
</code>
</pre>

양자화시에 낮은 비트로의 양자화를 진행할수록 정확성에 손실이 있을 수 있음을 주의하자

현재 허깅페이스에 gguf 모델이 있는경우는 해당 모델을 사용하고, 없을경우는 위를 통하여 직접 양자화 하였다.
